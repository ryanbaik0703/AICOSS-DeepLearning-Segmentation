{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy import ndimage\n",
    "\n",
    "from transformers import AutoProcessor, Qwen3VLForConditionalGeneration\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jjs2403/2026_bootcamp_02/models/sam2')\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 0) Seed / Device\n",
    "# =============================================================================\n",
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Paths (요청대로 경로 수정하지 않음)\n",
    "# =============================================================================\n",
    "BASE_DIR = Path('/home/jjs2403/2026_bootcamp_02')\n",
    "DATASET_DIR = BASE_DIR / 'dataset/challenge_datasets/challenge2'\n",
    "TEST_IMG_DIR = DATASET_DIR / 'input_images'\n",
    "\n",
    "OUTPUT_DIR = Path.home() / 'challenge2_outputs'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VIZ_FORCE_OVERWRITE = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Load Models (Qwen3 VL / SAM2.1)\n",
    "# =============================================================================\n",
    "MODEL_PATH = BASE_DIR / 'models/Qwen3-VL-8B-Instruct'\n",
    "vl_model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    str(MODEL_PATH),\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map='auto',\n",
    "    attn_implementation='flash_attention_2'\n",
    ")\n",
    "vl_processor = AutoProcessor.from_pretrained(str(MODEL_PATH))\n",
    "\n",
    "SAM2_1_MODEL_PATH = BASE_DIR / 'models/SAM2.1/weights/sam2.1_hiera_large.pt'\n",
    "SAM2_1_MODEL_CFG_PATH = BASE_DIR / 'models/SAM2.1/weights/sam2.1_hiera_l.yaml'\n",
    "\n",
    "checkpoint = torch.load(SAM2_1_MODEL_PATH, map_location=DEVICE)\n",
    "cfg = OmegaConf.load(SAM2_1_MODEL_CFG_PATH)\n",
    "sam_model = instantiate(cfg.model, _recursive_=True)\n",
    "sam_model.load_state_dict(checkpoint['model'])\n",
    "sam_model = sam_model.to(DEVICE).eval()\n",
    "sam_predictor = SAM2ImagePredictor(sam_model)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) Dataset / Loader\n",
    "# =============================================================================\n",
    "class MarineTestDataset(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_files = sorted([\n",
    "            f for f in os.listdir(img_dir)\n",
    "            if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        img_id = os.path.splitext(img_name)[0]\n",
    "        return image, img_id\n",
    "\n",
    "assert TEST_IMG_DIR.exists(), f\"TEST_IMG_DIR not found: {TEST_IMG_DIR}\"\n",
    "\n",
    "test_dataset = MarineTestDataset(img_dir=str(TEST_IMG_DIR))\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda x: x[0]\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4) Utils: clamp / robust JSON extraction\n",
    "# =============================================================================\n",
    "def clamp_int(v, lo, hi):\n",
    "    return int(max(lo, min(hi, int(round(v)))))\n",
    "\n",
    "def clamp_bbox_xyxy(bbox, w, h):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    x1 = clamp_int(x1, 0, w - 1)\n",
    "    y1 = clamp_int(y1, 0, h - 1)\n",
    "    x2 = clamp_int(x2, 0, w - 1)\n",
    "    y2 = clamp_int(y2, 0, h - 1)\n",
    "    if x2 < x1: x1, x2 = x2, x1\n",
    "    if y2 < y1: y1, y2 = y2, y1\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "def _strip_role_prefix(text: str) -> str:\n",
    "    t = text.strip()\n",
    "    if \"assistant\" in t:\n",
    "        t = t.split(\"assistant\", 1)[-1].strip()\n",
    "    t = re.sub(r\"^\\s*(user|system)\\s*:?\\s*\", \"\", t, flags=re.IGNORECASE).strip()\n",
    "    return t\n",
    "\n",
    "def _extract_first_json_like(text: str) -> str:\n",
    "    t = text.strip()\n",
    "    t = re.sub(r\"^```(?:json)?\", \"\", t, flags=re.IGNORECASE).strip()\n",
    "    t = re.sub(r\"```$\", \"\", t).strip()\n",
    "\n",
    "    m_obj = re.search(r\"\\{.*\\}\", t, flags=re.DOTALL)\n",
    "    m_arr = re.search(r\"\\[.*\\]\", t, flags=re.DOTALL)\n",
    "\n",
    "    if m_obj and m_arr:\n",
    "        cand = m_obj.group(0) if m_obj.start() < m_arr.start() else m_arr.group(0)\n",
    "    elif m_obj:\n",
    "        cand = m_obj.group(0)\n",
    "    elif m_arr:\n",
    "        cand = m_arr.group(0)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "    cand = re.sub(r\",\\s*([}\\]])\", r\"\\1\", cand)\n",
    "    return cand.strip()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5) Qwen Turn-1: ROI BBox\n",
    "# =============================================================================\n",
    "BBOX_PROMPT = \"\"\"You are given an image.\n",
    "Task: Find ONE large ROI bounding box that covers the main floating debris region on the water surface.\n",
    "This ROI should include the straw/debris mat area broadly (even if straw is not sharply visible).\n",
    "Include plastics / styrofoam / branches that are embedded in or near the straw mat.\n",
    "Exclude: sky, upper land/buildings, distant background.\n",
    "\n",
    "Important:\n",
    "- Make the box LARGE enough to cover the entire debris/straw region, not just a small object.\n",
    "- Prefer over-coverage rather than missing straw mat.\n",
    "\n",
    "Output ONLY one JSON object:\n",
    "{\"bbox_2d\":[x1,y1,x2,y2]}\n",
    "Coordinates must be in PIXELS (top-left origin).\n",
    "No extra text.\n",
    "\"\"\"\n",
    "\n",
    "def parse_bbox_from_text(text: str, w: int, h: int):\n",
    "    t = _strip_role_prefix(text)\n",
    "    js = _extract_first_json_like(t)\n",
    "    if not js:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(js)\n",
    "        if (\n",
    "            isinstance(obj, dict)\n",
    "            and \"bbox_2d\" in obj\n",
    "            and isinstance(obj[\"bbox_2d\"], (list, tuple))\n",
    "            and len(obj[\"bbox_2d\"]) >= 4\n",
    "        ):\n",
    "            bbox = [float(x) for x in obj[\"bbox_2d\"][:4]]\n",
    "            return clamp_bbox_xyxy(bbox, w, h)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    m = re.search(r\"\\[\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*\\]\", t)\n",
    "    if m:\n",
    "        bbox = [float(m.group(i)) for i in range(1, 5)]\n",
    "        return clamp_bbox_xyxy(bbox, w, h)\n",
    "\n",
    "    return None\n",
    "\n",
    "@torch.no_grad()\n",
    "def qwen_predict_roi_bbox(image: Image.Image):\n",
    "    messages = [{\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'image', 'image': image},\n",
    "            {'type': 'text', 'text': BBOX_PROMPT}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    text = vl_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = vl_processor(text=[text], images=[image], return_tensors='pt')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to('cuda') if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "\n",
    "    output_ids = vl_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        num_beams=1\n",
    "    )\n",
    "    out = vl_processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    out = _strip_role_prefix(out)\n",
    "\n",
    "    w, h = image.size\n",
    "    bbox = parse_bbox_from_text(out, w, h)\n",
    "    if bbox is None:\n",
    "        bbox = [0, int(h * 0.40), w - 1, h - 1]\n",
    "    return bbox, out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6) Qwen Turn-2: Points (pos/neg) inside ROI\n",
    "# =============================================================================\n",
    "LOW_COVERAGE_EXTRA = \"\"\"\n",
    "ADDITIONAL CONSTRAINT (SECOND PASS ONLY):\n",
    "- The final mask must cover LESS THAN 30% of the entire image.\n",
    "- Avoid placing POS points that could make the mask overly large.\n",
    "- Use NEG points to block broad water areas and large non-target regions.\n",
    "- Focus POS on compact, dense straw/debris clusters only.\n",
    "\"\"\"\n",
    "\n",
    "POINTS_PROMPT_TEMPLATE = \"\"\"You are given an image and an ROI bounding box in pixels: ROI=[{x1},{y1},{x2},{y2}].\n",
    "\n",
    "Inside this ROI, choose points for segmentation guidance for a segmentation model (SAM).\n",
    "\n",
    "TARGET (POS): the straw/debris mat floating on the water surface (broad region), including embedded small debris.\n",
    "- If pure-white rigid debris (styrofoam blocks / white plastic chunks) is present, you MUST place at least 2 POS points on it (required). These white objects are NOT glare and must be included in the final mask. If none are visible, do not invent them.\n",
    "- Provide POS points on the thick interior of the largest continuous straw/debris mat region.\n",
    "- Prefer dense straw-like texture (not smooth water reflection).\n",
    "- If possible, keep POS points ~50-80 pixels away from the straw-mat boundary.\n",
    "- Avoid mixed boundary zones (water + straw edges) and avoid isolated tiny debris pieces.\n",
    "\n",
    "NON-TARGET (NEG): areas that should NOT be segmented as straw/debris mat.\n",
    "PRIORITY (most important): SUN-GLARE / SPECULAR REFLECTION WATER\n",
    "- Place most NEG points on shiny glare patches on water (bright reflection streaks, mirror-like highlights).\n",
    "- These glare regions are smooth reflective water, NOT straw texture.\n",
    "\n",
    "SECONDARY NEG (only if needed and clearly non-target):\n",
    "- Sea foam / bubbly water patches (foam texture)\n",
    "- Dark shadow water near walls/rocks\n",
    "- Wall/tires/rocks texture (if inside ROI)\n",
    "\n",
    "IMPORTANT EXCLUSION for NEG:\n",
    "- Do NOT place NEG points on pure-white rigid debris objects such as styrofoam blocks or white plastic chunks.\n",
    "  (They are debris and should remain segmentable; if visible, POS on them is REQUIRED and the final mask must include them.)\n",
    "\n",
    "COUNTS:\n",
    "- Provide exactly 4 POS points.\n",
    "- Provide exactly 6 NEG points.\n",
    "- List neg_points in priority order (most important first).\n",
    "- All points must be inside the ROI.\n",
    "- Use full-image pixel coordinates (not cropped coordinates).\n",
    "\n",
    "OUTPUT (STRICT JSON ONLY, no extra text):\n",
    "{{\"pos_points\":[[x,y],[x,y],[x,y],[x,y]], \"neg_points\":[[x,y],[x,y],[x,y],[x,y],[x,y],[x,y]]}}\n",
    "\"\"\"\n",
    "\n",
    "def parse_points_from_text(text: str, bbox, w: int, h: int):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    t = _strip_role_prefix(text)\n",
    "    js = _extract_first_json_like(t)\n",
    "    if not js:\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(js)\n",
    "        pos = obj.get(\"pos_points\", [])\n",
    "        neg = obj.get(\"neg_points\", [])\n",
    "        if not (isinstance(pos, list) and isinstance(neg, list)):\n",
    "            return None, None\n",
    "\n",
    "        def _clamp_pt(pt):\n",
    "            px, py = pt\n",
    "            px = clamp_int(px, x1, x2)\n",
    "            py = clamp_int(py, y1, y2)\n",
    "            return [px, py]\n",
    "\n",
    "        pos2 = []\n",
    "        for p in pos:\n",
    "            if isinstance(p, (list, tuple)) and len(p) == 2:\n",
    "                pos2.append(_clamp_pt(p))\n",
    "\n",
    "        neg2 = []\n",
    "        for p in neg:\n",
    "            if isinstance(p, (list, tuple)) and len(p) == 2:\n",
    "                neg2.append(_clamp_pt(p))\n",
    "\n",
    "        rng = np.random.default_rng(42)\n",
    "\n",
    "        def _random_pt():\n",
    "            return [int(rng.integers(x1, x2 + 1)), int(rng.integers(y1, y2 + 1))]\n",
    "\n",
    "        while len(pos2) < 4:\n",
    "            pos2.append(_random_pt())\n",
    "        pos2 = pos2[:4]\n",
    "\n",
    "        while len(neg2) < 6:\n",
    "            neg2.append(_random_pt())\n",
    "        neg2 = neg2[:6]\n",
    "\n",
    "        return pos2, neg2\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "@torch.no_grad()\n",
    "def qwen_predict_points(image: Image.Image, bbox, extra_instruction: str = None):\n",
    "    w, h = image.size\n",
    "    x1, y1, x2, y2 = bbox\n",
    "\n",
    "    prompt = POINTS_PROMPT_TEMPLATE.format(x1=x1, y1=y1, x2=x2, y2=y2)\n",
    "    if extra_instruction:\n",
    "        prompt = prompt + \"\\n\\n\" + extra_instruction\n",
    "\n",
    "    messages = [{\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'image', 'image': image},\n",
    "            {'type': 'text', 'text': prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    text = vl_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = vl_processor(text=[text], images=[image], return_tensors='pt')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to('cuda') if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "\n",
    "    output_ids = vl_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=384,\n",
    "        do_sample=False,\n",
    "        num_beams=1\n",
    "    )\n",
    "    out = vl_processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    out_clean = _strip_role_prefix(out)\n",
    "\n",
    "    pos, neg = parse_points_from_text(out_clean, bbox, w, h)\n",
    "    if pos is None or neg is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        pos = [[int(rng.integers(x1, x2 + 1)), int(rng.integers(y1, y2 + 1))] for _ in range(4)]\n",
    "        neg = [[int(rng.integers(x1, x2 + 1)), int(rng.integers(y1, y2 + 1))] for _ in range(6)]\n",
    "\n",
    "    return pos, neg, out_clean\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7) SAM: box + points -> multimask, select best (area gate + point consistency)\n",
    "# =============================================================================\n",
    "def mask_point_hits(mask: np.ndarray, pts):\n",
    "    if len(pts) == 0:\n",
    "        return 0.0\n",
    "    h, w = mask.shape\n",
    "    hits = 0\n",
    "    for x, y in pts:\n",
    "        x = clamp_int(x, 0, w - 1)\n",
    "        y = clamp_int(y, 0, h - 1)\n",
    "        hits += 1 if mask[y, x] else 0\n",
    "    return hits / max(len(pts), 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sam_segment_box_points(\n",
    "    image: Image.Image, bbox, pos_pts, neg_pts,\n",
    "    min_area_ratio=0.003, max_area_ratio=0.95,\n",
    "    pos_miss_weight=2.0, neg_hit_weight=6.0,\n",
    "    min_pos_hit=1.0, force_pos=True\n",
    "):\n",
    "    img = np.array(image, dtype=np.uint8)\n",
    "    H, W = img.shape[:2]\n",
    "    sam_predictor.set_image(img)\n",
    "\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi_area = float(max(1, (x2 - x1 + 1) * (y2 - y1 + 1)))\n",
    "\n",
    "    all_pts = np.array(pos_pts + neg_pts, dtype=np.float32)\n",
    "    all_labels = np.array([1] * len(pos_pts) + [0] * len(neg_pts), dtype=np.int32)\n",
    "    box = np.array(bbox, dtype=np.float32)[None, :]\n",
    "\n",
    "    masks, scores, _ = sam_predictor.predict(\n",
    "        box=box,\n",
    "        point_coords=all_pts[None, :, :],\n",
    "        point_labels=all_labels[None, :],\n",
    "        multimask_output=True\n",
    "    )\n",
    "\n",
    "    best_mask = None\n",
    "    best_val = -1e18\n",
    "    best_pos_mask = None\n",
    "    best_pos_hit = -1.0\n",
    "    best_pos_score = -1e18\n",
    "\n",
    "    for i in range(masks.shape[0]):\n",
    "        m = masks[i].astype(bool)\n",
    "        s = float(scores[i])\n",
    "\n",
    "        roi_m = m[y1:y2 + 1, x1:x2 + 1]\n",
    "        area_ratio = float(roi_m.sum()) / roi_area\n",
    "\n",
    "        if area_ratio < min_area_ratio or area_ratio > max_area_ratio:\n",
    "            continue\n",
    "\n",
    "        pos_hit = mask_point_hits(m, pos_pts)\n",
    "        neg_hit = mask_point_hits(m, neg_pts)\n",
    "        pos_miss = 1.0 - pos_hit\n",
    "\n",
    "        if pos_hit > best_pos_hit or (pos_hit == best_pos_hit and s > best_pos_score):\n",
    "            best_pos_hit = pos_hit\n",
    "            best_pos_score = s\n",
    "            best_pos_mask = m\n",
    "\n",
    "        if force_pos and pos_hit < min_pos_hit:\n",
    "            continue\n",
    "\n",
    "        val = s - pos_miss_weight * pos_miss - neg_hit_weight * neg_hit\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best_mask = m\n",
    "\n",
    "    if best_mask is None and best_pos_mask is not None:\n",
    "        best_mask = best_pos_mask\n",
    "\n",
    "    if best_mask is None:\n",
    "        idx = int(np.argmax(scores))\n",
    "        best_mask = masks[idx].astype(bool)\n",
    "\n",
    "    return best_mask\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8) Post-processing\n",
    "# =============================================================================\n",
    "def clean_mask_components(mask: np.ndarray, min_area=40):\n",
    "    labeled, num = ndimage.label(mask)\n",
    "    out = np.zeros_like(mask, dtype=bool)\n",
    "    for label_id in range(1, num + 1):\n",
    "        comp = (labeled == label_id)\n",
    "        if int(comp.sum()) >= int(min_area):\n",
    "            out |= comp\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 9) RLE Encoding\n",
    "# =============================================================================\n",
    "def rle_encode(mask):\n",
    "    if torch.is_tensor(mask):\n",
    "        mask = mask.detach().cpu().numpy()\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[False], pixels, [False]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 10) Visualization (No OpenCV)\n",
    "# =============================================================================\n",
    "def save_viz(image: Image.Image, img_id: str, bbox, pos_pts, neg_pts, mask: np.ndarray):\n",
    "    img = np.array(image)\n",
    "    H, W = img.shape[:2]\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    coverage = float(mask.sum()) / float(mask.size) * 100.0\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "    axes[0].imshow(img)\n",
    "    title = f'{img_id} - ROI bbox + points'\n",
    "    if RUN_TAG:\n",
    "        title += f'\\nrun {RUN_TAG}'\n",
    "    axes[0].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    import matplotlib.patches as patches\n",
    "    rect = patches.Rectangle((x1, y1), (x2 - x1 + 1), (y2 - y1 + 1),\n",
    "                             linewidth=2.5, edgecolor='lime', facecolor='none')\n",
    "    axes[0].add_patch(rect)\n",
    "\n",
    "    if len(pos_pts) > 0:\n",
    "        px = [p[0] for p in pos_pts]\n",
    "        py = [p[1] for p in pos_pts]\n",
    "        axes[0].scatter(px, py, s=60, c='dodgerblue', marker='o', label='POS')\n",
    "    if len(neg_pts) > 0:\n",
    "        nx = [p[0] for p in neg_pts]\n",
    "        ny = [p[1] for p in neg_pts]\n",
    "        axes[0].scatter(nx, ny, s=60, c='orange', marker='x', label='NEG')\n",
    "\n",
    "    axes[0].legend(loc='lower right')\n",
    "\n",
    "    axes[1].imshow(img)\n",
    "    axes[1].imshow(mask, alpha=0.55)\n",
    "    axes[1].set_title('SAM Segmentation (overlay)', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(mask, cmap='gray')\n",
    "    axes[2].set_title(f'Mask ({coverage:.2f}%)', fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path = OUTPUT_DIR / f'viz_{img_id}.png'\n",
    "    if VIZ_FORCE_OVERWRITE and out_path.exists():\n",
    "        out_path.unlink()\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 11) Main loop: ROI BBox -> Points -> SAM -> Postproc -> Viz\n",
    "# =============================================================================\n",
    "mask_results = []\n",
    "debug_rows = []\n",
    "viz_paths = []\n",
    "\n",
    "for image, img_id in tqdm(test_loader, desc='Segmentation'):\n",
    "    w, h = image.size\n",
    "    try:\n",
    "        # 1) ROI bbox\n",
    "        bbox, bbox_raw = qwen_predict_roi_bbox(image)\n",
    "\n",
    "        # 2) Points inside ROI\n",
    "        pos_pts, neg_pts, points_raw = qwen_predict_points(image, bbox)\n",
    "\n",
    "        # 3) SAM segment with box+points\n",
    "        mask = sam_segment_box_points(\n",
    "            image=image,\n",
    "            bbox=bbox,\n",
    "            pos_pts=pos_pts,\n",
    "            neg_pts=neg_pts,\n",
    "            # ---- 튜닝 포인트 ----\n",
    "            min_area_ratio=0.003,\n",
    "            max_area_ratio=0.80,\n",
    "            pos_miss_weight=2.0,\n",
    "            neg_hit_weight=2.5,\n",
    "            min_pos_hit=1.0,\n",
    "            force_pos=True\n",
    "        )\n",
    "        mask = clean_mask_components(mask, min_area=40)\n",
    "\n",
    "        coverage = float(mask.sum()) / float(mask.size) * 100.0\n",
    "\n",
    "        # Retry once if coverage is too high\n",
    "        retry_used = False\n",
    "        coverage_retry = None\n",
    "        points_raw_retry = None\n",
    "\n",
    "        if coverage >= 30.0:\n",
    "            retry_used = True\n",
    "            pos_pts, neg_pts, points_raw_retry = qwen_predict_points(\n",
    "                image, bbox, extra_instruction=LOW_COVERAGE_EXTRA\n",
    "            )\n",
    "            mask = sam_segment_box_points(\n",
    "                image=image,\n",
    "                bbox=bbox,\n",
    "                pos_pts=pos_pts,\n",
    "                neg_pts=neg_pts,\n",
    "                # ---- 튜닝 포인트 ----\n",
    "                min_area_ratio=0.003,\n",
    "                max_area_ratio=0.97,\n",
    "                pos_miss_weight=2.0,\n",
    "                neg_hit_weight=6.0,\n",
    "                min_pos_hit=1.0,\n",
    "                force_pos=True\n",
    "            )\n",
    "            mask = clean_mask_components(mask, min_area=40)\n",
    "            coverage = float(mask.sum()) / float(mask.size) * 100.0\n",
    "            coverage_retry = coverage\n",
    "\n",
    "        mask_results.append({'ID': img_id, 'mask': mask})\n",
    "\n",
    "        debug_rows.append({\n",
    "            'ID': img_id,\n",
    "            'bbox': bbox,\n",
    "            'pos_pts': pos_pts,\n",
    "            'neg_pts': neg_pts,\n",
    "            'coverage_pct': coverage,\n",
    "            'retry_used': retry_used,\n",
    "            'coverage_retry': coverage_retry,\n",
    "            'bbox_raw_head': bbox_raw[:180],\n",
    "            'points_raw_head': points_raw[:180],\n",
    "            'points_raw_head_retry': (points_raw_retry[:180] if points_raw_retry else None),\n",
    "        })\n",
    "\n",
    "        vp = save_viz(image, img_id, bbox, pos_pts, neg_pts, mask)\n",
    "        viz_paths.append(str(vp))\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        mask_results.append({'ID': img_id, 'mask': np.zeros((h, w), dtype=bool)})\n",
    "        debug_rows.append({'ID': img_id, 'bbox': None, 'pos_pts': None, 'neg_pts': None, 'coverage_pct': 0.0})\n",
    "\n",
    "mask_dict = {r['ID']: r['mask'] for r in mask_results}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 12) Submission + Debug dump + Stats\n",
    "# =============================================================================\n",
    "submission_data = []\n",
    "for r in mask_results:\n",
    "    img_id = r['ID']\n",
    "    mask = r['mask']\n",
    "    submission_data.append({'ID': img_id, 'Label': rle_encode(mask)})\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data, columns=['ID', 'Label'])\n",
    "submission_path = OUTPUT_DIR / 'submission_qwen_roi_points_sam.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "debug_path = OUTPUT_DIR / 'debug_qwen_roi_points.json'\n",
    "with open(debug_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(debug_rows, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "coverages = [d.get('coverage_pct', 0.0) for d in debug_rows if isinstance(d.get('coverage_pct', None), (int, float))]\n",
    "if coverages:\n",
    "    _avg = float(np.mean(coverages))\n",
    "    _min = float(np.min(coverages))\n",
    "    _max = float(np.max(coverages))\n",
    "\n",
    "# (필요하면 아래만 출력해서 최소 로그로 확인)\n",
    "print(f\"Saved: {submission_path}\")\n",
    "print(f\"Saved: {debug_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
