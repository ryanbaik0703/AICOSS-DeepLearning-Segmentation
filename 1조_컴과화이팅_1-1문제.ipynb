{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02913a9-9f6f-43be-8f43-9bd6da69c836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ ê²½ì§„ëŒ€íšŒ ë¬¸ì œ 1-1\n",
      "================================================================================\n",
      "PyTorch: 2.6.0+cu118\n",
      "CUDA: True\n",
      "âœ… Seed fixed: 42\n",
      "\n",
      "ğŸ“ Dataset: /home/jjs2403/2026_bootcamp_02/dataset/challenge_datasets/challenge1\n",
      "ğŸ“ Output: /home/bootcamp2026_01/challenge1_outputs\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6946d33b9214805aefc9ed0b5634e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ì´ë¯¸ì§€ ë¡œë”©:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ë¯¸ì§€: 129ì¥\n",
      "âœ… ì¿¼ë¦¬: 129ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ¤– MLLM ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e11f2d5d354d63b41d23ef928e470e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MLLM ë¡œë”© ì™„ë£Œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ SAM2.1 ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      "================================================================================\n",
      "âœ… SAM2.1 ë¡œë”© ì™„ë£Œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ BBox ì˜ˆì¸¡ ì‹œì‘ (Visual Focus Refinement)...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3669ade577174f4ba0acfc5a6e35b636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BBox ì˜ˆì¸¡:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… BBox ì˜ˆì¸¡ ì™„ë£Œ: 129/129\n",
      "   Fallback ì‚¬ìš©: 2ê°œ\n",
      "   Plural debug saved: /home/bootcamp2026_01/challenge1_outputs/plural_debug.csv (rows: 41)\n",
      "\n",
      "================================================================================\n",
      "ğŸ­ Mask ìƒì„± ì‹œì‘ (multimask)...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897bbc0352d24a778060145e3a504e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mask ìƒì„±:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== ê²½ì§„ëŒ€íšŒ ë¬¸ì œ 1-1: ìµœì¢… ê°œì„  ë²„ì „ =====\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoProcessor, Qwen3VLForConditionalGeneration\n",
    "\n",
    "# SAM2.1\n",
    "import sys\n",
    "sys.path.append('/home/jjs2403/2026_bootcamp_02/models/sam2')\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸš€ ê²½ì§„ëŒ€íšŒ ë¬¸ì œ 1-1\")\n",
    "print(\"=\"*80)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "# ===== 1. Seed ê³ ì • =====\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\"âœ… Seed fixed: {seed}\")\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# ===== 2. ê²½ë¡œ ì„¤ì • =====\n",
    "BASE_DIR = Path(\"/home/jjs2403/2026_bootcamp_02\")\n",
    "DATASET_DIR = BASE_DIR / \"dataset/challenge_datasets/challenge1\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "OUTPUT_DIR = Path.home() / \"challenge1_outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Multi-box safety (only for explicit plural queries)\n",
    "MAX_MULTI_BOXES = 4\n",
    "NEGATIVE_OVERSEG_BOX_RATIO = 0.95\n",
    "NEGATIVE_OVERSEG_IMG_RATIO = 0.60\n",
    "PLURAL_DEBUG_CSV = OUTPUT_DIR / \"plural_debug.csv\"\n",
    "\n",
    "print()\n",
    "print(f\"ğŸ“ Dataset: {DATASET_DIR}\")\n",
    "print(f\"ğŸ“ Output: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# ===== 3. ë°ì´í„°ì…‹ ë¡œë”© =====\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_img_dir = DATASET_DIR / \"test\"\n",
    "image_files = sorted(test_img_dir.glob(\"*.jpg\")) + sorted(test_img_dir.glob(\"*.png\"))\n",
    "\n",
    "test_images = {}\n",
    "for img_path in tqdm(image_files, desc=\"ì´ë¯¸ì§€ ë¡œë”©\"):\n",
    "    img_id = img_path.stem\n",
    "    test_images[img_id] = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "query_csv = DATASET_DIR / \"query.csv\"\n",
    "query_df = pd.read_csv(query_csv)\n",
    "\n",
    "id_col = next((col for col in query_df.columns if col.lower() in ['id', 'image_id', 'filename']), query_df.columns[0])\n",
    "query_col = next((col for col in query_df.columns if col.lower() in ['query', 'question', 'text']), query_df.columns[1])\n",
    "\n",
    "queries = {}\n",
    "for _, row in query_df.iterrows():\n",
    "    img_id = str(row[id_col]).replace('.jpg', '').replace('.png', '')\n",
    "    queries[img_id] = row[query_col]\n",
    "\n",
    "print(f\"âœ… ì´ë¯¸ì§€: {len(test_images)}ì¥\")\n",
    "print(f\"âœ… ì¿¼ë¦¬: {len(queries)}ê°œ\")\n",
    "\n",
    "\n",
    "# ===== 4. ëª¨ë¸ ë¡œë”© =====\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¤– MLLM ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_path = MODEL_DIR / \"Qwen3-VL-8B-Instruct\"\n",
    "mllm_processor = AutoProcessor.from_pretrained(str(model_path))\n",
    "mllm_model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    str(model_path),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"âœ… MLLM ë¡œë”© ì™„ë£Œ\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¯ SAM2.1 ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checkpoint_path = \"/home/jjs2403/2026_bootcamp_02/models/SAM2.1/weights/sam2.1_hiera_large.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "config_path = \"/home/jjs2403/2026_bootcamp_02/models/SAM2.1/weights/sam2.1_hiera_l.yaml\"\n",
    "cfg = OmegaConf.load(config_path)\n",
    "\n",
    "sam_model = instantiate(cfg.model, _recursive_=True)\n",
    "sam_model.load_state_dict(checkpoint[\"model\"])\n",
    "sam_model = sam_model.to(DEVICE).eval()\n",
    "\n",
    "sam_predictor = SAM2ImagePredictor(sam_model)\n",
    "print(f\"âœ… SAM2.1 ë¡œë”© ì™„ë£Œ\")\n",
    "\n",
    "\n",
    "# ===== 5. BBox íŒŒì‹± í•¨ìˆ˜ =====\n",
    "def parse_bbox_from_text(text: str, img_width: int, img_height: int):\n",
    "    # 1) JSON ë¸”ë¡ ì¶”ì¶œ\n",
    "    json_blocks = re.findall(r\"\\{.*?\\}\", text, flags=re.DOTALL)\n",
    "    for blk in json_blocks:\n",
    "        try:\n",
    "            obj = json.loads(blk)\n",
    "            if \"bbox_2d\" in obj and isinstance(obj[\"bbox_2d\"], list) and len(obj[\"bbox_2d\"]) >= 4:\n",
    "                bbox = [float(x) for x in obj[\"bbox_2d\"][:4]]\n",
    "                return clamp_bbox(bbox, img_width, img_height)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # 2) ë¦¬ìŠ¤íŠ¸ í˜•ì‹ [x1,y1,x2,y2]\n",
    "    m = re.search(r\"\\[\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*\\]\", text)\n",
    "    if m:\n",
    "        bbox = [float(m.group(i)) for i in range(1, 5)]\n",
    "        return clamp_bbox(bbox, img_width, img_height)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def clamp_bbox(bbox, w, h):\n",
    "    \"\"\"BBoxë¥¼ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ì¡°ì •\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    x1 = int(max(0, min(w - 1, round(x1))))\n",
    "    y1 = int(max(0, min(h - 1, round(y1))))\n",
    "    x2 = int(max(0, min(w - 1, round(x2))))\n",
    "    y2 = int(max(0, min(h - 1, round(y2))))\n",
    "    if x2 < x1:\n",
    "        x1, x2 = x2, x1\n",
    "    if y2 < y1:\n",
    "        y1, y2 = y2, y1\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "\n",
    "def is_plural_query(query: str) -> bool:\n",
    "    q = str(query).lower() if query is not None else \"\"\n",
    "    if re.search(r\"\\b(all|every|both|each|among|several|multiple|various)\\b\", q):\n",
    "        return True\n",
    "    if re.search(r\"\\b(two|three|four|five|six|seven|eight|nine|ten)\\b\", q):\n",
    "        return True\n",
    "    plural_nouns = [\n",
    "        \"people\", \"men\", \"women\", \"children\", \"kids\",\n",
    "        \"players\", \"targets\",\n",
    "        \"vehicles\", \"cars\", \"bikes\", \"boats\", \"trains\",\n",
    "        \"dice\", \"fruits\",\n",
    "        \"items\", \"objects\", \"boxes\",\n",
    "        \"buttons\", \"ports\", \"parts\", \"signs\",\n",
    "        \"labels\", \"texts\", \"bars\", \"lines\", \"cells\", \"years\",\n",
    "        \"devices\",\n",
    "    ]\n",
    "    return any(re.search(rf\"\\b{re.escape(k)}\\b\", q) for k in plural_nouns)\n",
    "\n",
    "\n",
    "def bbox_iou(box_a: List[int], box_b: List[int]) -> float:\n",
    "    ax1, ay1, ax2, ay2 = box_a\n",
    "    bx1, by1, bx2, by2 = box_b\n",
    "\n",
    "    ix1 = max(ax1, bx1)\n",
    "    iy1 = max(ay1, by1)\n",
    "    ix2 = min(ax2, bx2)\n",
    "    iy2 = min(ay2, by2)\n",
    "\n",
    "    iw = max(0, ix2 - ix1 + 1)\n",
    "    ih = max(0, iy2 - iy1 + 1)\n",
    "    inter = iw * ih\n",
    "\n",
    "    area_a = max(0, ax2 - ax1 + 1) * max(0, ay2 - ay1 + 1)\n",
    "    area_b = max(0, bx2 - bx1 + 1) * max(0, by2 - by1 + 1)\n",
    "    union = area_a + area_b - inter\n",
    "    if union <= 0:\n",
    "        return 0.0\n",
    "    return inter / union\n",
    "\n",
    "\n",
    "def dedupe_bboxes(bboxes: List[List[int]]) -> List[List[int]]:\n",
    "    kept: List[List[int]] = []\n",
    "    for bbox in bboxes:\n",
    "        if all(bbox_iou(bbox, k) < 0.7 for k in kept):\n",
    "            kept.append(bbox)\n",
    "        if len(kept) >= MAX_MULTI_BOXES:\n",
    "            break\n",
    "    return kept\n",
    "\n",
    "\n",
    "def parse_bboxes_from_text(text: str, img_width: int, img_height: int) -> List[List[int]]:\n",
    "    bboxes: List[List[int]] = []\n",
    "    json_blocks = re.findall(r\"\\{.*?\\}\", text, flags=re.DOTALL)\n",
    "    for blk in json_blocks:\n",
    "        try:\n",
    "            obj = json.loads(blk)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if \"bboxes_2d\" in obj and isinstance(obj[\"bboxes_2d\"], list):\n",
    "            for item in obj[\"bboxes_2d\"]:\n",
    "                if isinstance(item, list) and len(item) >= 4:\n",
    "                    bbox = [float(x) for x in item[:4]]\n",
    "                    bboxes.append(clamp_bbox(bbox, img_width, img_height))\n",
    "            if bboxes:\n",
    "                return bboxes\n",
    "\n",
    "        if \"bbox_2d\" in obj and isinstance(obj[\"bbox_2d\"], list) and len(obj[\"bbox_2d\"]) >= 4:\n",
    "            bbox = [float(x) for x in obj[\"bbox_2d\"][:4]]\n",
    "            bboxes.append(clamp_bbox(bbox, img_width, img_height))\n",
    "            return bboxes\n",
    "\n",
    "    m_all = re.findall(r\"\\[\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*,\\s*([\\d.]+)\\s*\\]\", text)\n",
    "    for m in m_all:\n",
    "        bbox = [float(m[0]), float(m[1]), float(m[2]), float(m[3])]\n",
    "        bboxes.append(clamp_bbox(bbox, img_width, img_height))\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# ===== 6. Visual Focus =====\n",
    "def apply_visual_focus(image: Image.Image, bbox: List[int]) -> Image.Image:\n",
    "    \"\"\"Blur + darken background, keep bbox area sharp and bright.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    w, h = image.size\n",
    "\n",
    "    x1 = max(0, min(w - 1, x1))\n",
    "    y1 = max(0, min(h - 1, y1))\n",
    "    x2 = max(0, min(w - 1, x2))\n",
    "    y2 = max(0, min(h - 1, y2))\n",
    "    if x2 < x1:\n",
    "        x1, x2 = x2, x1\n",
    "    if y2 < y1:\n",
    "        y1, y2 = y2, y1\n",
    "\n",
    "    blurred = image.filter(ImageFilter.GaussianBlur(radius=10))\n",
    "    darkened = ImageEnhance.Brightness(blurred).enhance(0.5)\n",
    "    focused = darkened.copy()\n",
    "\n",
    "    patch = image.crop((x1, y1, x2 + 1, y2 + 1))\n",
    "    focused.paste(patch, (x1, y1))\n",
    "    return focused\n",
    "\n",
    "\n",
    "# ===== 7. BBox ì˜ˆì¸¡ (0.49 í”„ë¡¬í”„íŠ¸ ì ìš©) =====\n",
    "@torch.no_grad()\n",
    "def predict_bbox_with_mllm(\n",
    "    model,\n",
    "    processor,\n",
    "    image: Image.Image,\n",
    "    query: str,\n",
    "    plural: bool\n",
    ") -> List[List[int]]:\n",
    "\n",
    "    w, h = image.size\n",
    "\n",
    "    if plural:\n",
    "        prompt = f\"\"\"You are given an image and a text query.\n",
    "Infer all target objects/regions that best satisfy the query.\n",
    "Output ONLY one JSON object with key bboxes_2d=[[x1,y1,x2,y2], ...] in pixel coordinates.\n",
    "No extra text.\n",
    "Query: {query}\n",
    "\"\"\".strip()\n",
    "    else:\n",
    "        # JSON í˜•ì‹ + single target + í”½ì…€ ì¢Œí‘œ\n",
    "        prompt = f\"\"\"You are given an image and a text query.\n",
    "Infer the single target object/region that best satisfies the query.\n",
    "Output ONLY one JSON object with key bbox_2d=[x1,y1,x2,y2] in pixel coordinates.\n",
    "No extra text.\n",
    "Query: {query}\n",
    "\"\"\".strip()\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") if hasattr(v, \"to\") else v for k, v in inputs.items()}\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False, \n",
    "        num_beams=1\n",
    "    )\n",
    "\n",
    "    out_text = processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "    if plural:\n",
    "        bboxes = parse_bboxes_from_text(out_text, w, h)\n",
    "        if not bboxes:\n",
    "            bboxes = [[0, 0, w - 1, h - 1]]\n",
    "        return dedupe_bboxes(bboxes)\n",
    "\n",
    "    bbox = parse_bbox_from_text(out_text, w, h)\n",
    "    if bbox is None:\n",
    "        bbox = [0, 0, w - 1, h - 1]\n",
    "    return [bbox]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_bbox_refine(\n",
    "    model,\n",
    "    processor,\n",
    "    image: Image.Image,\n",
    "    query: str\n",
    ") -> Tuple[List[List[int]], bool]:\n",
    "    \"\"\"Two-pass refinement with visual focus (no cropping).\"\"\"\n",
    "    w, h = image.size\n",
    "    fallback = [0, 0, w - 1, h - 1]\n",
    "    plural = is_plural_query(query)\n",
    "\n",
    "    bboxes_coarse = predict_bbox_with_mllm(model, processor, image, query, plural)\n",
    "    used_fallback = (len(bboxes_coarse) == 1 and bboxes_coarse[0] == fallback)\n",
    "    if used_fallback:\n",
    "        return bboxes_coarse, True\n",
    "\n",
    "    if plural:\n",
    "        return bboxes_coarse, False\n",
    "\n",
    "    focused_image = apply_visual_focus(image, bboxes_coarse[0])\n",
    "    bboxes_refined = predict_bbox_with_mllm(model, processor, focused_image, query, plural)\n",
    "\n",
    "    if len(bboxes_refined) == 1 and bboxes_refined[0] == fallback:\n",
    "        return bboxes_coarse, False\n",
    "\n",
    "    return bboxes_refined, False\n",
    "\n",
    "\n",
    "# ===== 8. SAM Mask ìƒì„± (multimask) =====\n",
    "@torch.no_grad()\n",
    "def generate_mask_with_sam(\n",
    "    sam_predictor,\n",
    "    image: Image.Image,\n",
    "    bbox: List[int],\n",
    "    use_points: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    multimask_output=True + ìµœê³  ì„ íƒ\n",
    "    \"\"\"\n",
    "    img_array = np.array(image, dtype=np.uint8)\n",
    "    sam_predictor.set_image(img_array)\n",
    "\n",
    "    box = np.array(bbox, dtype=np.float32)\n",
    "\n",
    "    if not use_points:\n",
    "        masks, scores, _ = sam_predictor.predict(\n",
    "            box=box[None, :],\n",
    "            multimask_output=True\n",
    "        )\n",
    "        best_idx = int(np.argmax(scores))\n",
    "        return masks[best_idx].astype(bool)\n",
    "\n",
    "    cx = (bbox[0] + bbox[2]) / 2.0\n",
    "    cy = (bbox[1] + bbox[3]) / 2.0\n",
    "    point_coords = np.array([[cx, cy]], dtype=np.float32)\n",
    "    point_labels = np.array([1], dtype=np.int32)\n",
    "\n",
    "    masks, scores, _ = sam_predictor.predict(\n",
    "        box=box[None, :],\n",
    "        point_coords=point_coords,\n",
    "        point_labels=point_labels,\n",
    "        multimask_output=True\n",
    "    )\n",
    "\n",
    "    best_idx = int(np.argmax(scores))\n",
    "    best_mask = masks[best_idx].astype(bool)\n",
    "\n",
    "    mask_area = int(best_mask.sum())\n",
    "    box_area = max(1, (bbox[2] - bbox[0]) * (bbox[3] - bbox[1]))\n",
    "    img_area = max(1, img_array.shape[0] * img_array.shape[1])\n",
    "    ratio_box = mask_area / float(box_area)\n",
    "    ratio_img = mask_area / float(img_area)\n",
    "\n",
    "    if ratio_box > NEGATIVE_OVERSEG_BOX_RATIO or ratio_img > NEGATIVE_OVERSEG_IMG_RATIO:\n",
    "        tight_bbox = mask_to_bbox(best_mask)\n",
    "        if tight_bbox is not None:\n",
    "            tight_box = np.array(tight_bbox, dtype=np.float32)\n",
    "            masks_t, scores_t, _ = sam_predictor.predict(\n",
    "                box=tight_box[None, :],\n",
    "                point_coords=point_coords,\n",
    "                point_labels=point_labels,\n",
    "                multimask_output=True\n",
    "            )\n",
    "            best_mask = masks_t[int(np.argmax(scores_t))].astype(bool)\n",
    "            mask_area = int(best_mask.sum())\n",
    "            ratio_box = mask_area / float(box_area)\n",
    "            ratio_img = mask_area / float(img_area)\n",
    "\n",
    "    if ratio_box > NEGATIVE_OVERSEG_BOX_RATIO or ratio_img > NEGATIVE_OVERSEG_IMG_RATIO:\n",
    "        neg = select_negative_point(best_mask, bbox)\n",
    "        if neg is not None:\n",
    "            point_coords = np.array([[cx, cy], neg], dtype=np.float32)\n",
    "            point_labels = np.array([1, 0], dtype=np.int32)\n",
    "            masks2, scores2, _ = sam_predictor.predict(\n",
    "                box=box[None, :],\n",
    "                point_coords=point_coords,\n",
    "                point_labels=point_labels,\n",
    "                multimask_output=True\n",
    "            )\n",
    "            best2 = masks2[int(np.argmax(scores2))].astype(bool)\n",
    "            return best2\n",
    "\n",
    "    return best_mask\n",
    "\n",
    "\n",
    "def select_negative_point(mask: np.ndarray, bbox: List[int]) -> Optional[List[float]]:\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    h, w = mask.shape[:2]\n",
    "    x1 = max(0, min(w - 1, x1))\n",
    "    y1 = max(0, min(h - 1, y1))\n",
    "    x2 = max(0, min(w - 1, x2))\n",
    "    y2 = max(0, min(h - 1, y2))\n",
    "\n",
    "    bbox_mask = np.zeros_like(mask, dtype=bool)\n",
    "    bbox_mask[y1 : y2 + 1, x1 : x2 + 1] = True\n",
    "    candidates = np.argwhere(bbox_mask & ~mask)\n",
    "    if candidates.size == 0:\n",
    "        return None\n",
    "    idx = len(candidates) // 2\n",
    "    y, x = candidates[idx]\n",
    "    return [float(x), float(y)]\n",
    "\n",
    "\n",
    "def mask_to_bbox(mask: np.ndarray) -> Optional[List[int]]:\n",
    "    ys, xs = np.where(mask)\n",
    "    if xs.size == 0 or ys.size == 0:\n",
    "        return None\n",
    "    x1 = int(xs.min())\n",
    "    y1 = int(ys.min())\n",
    "    x2 = int(xs.max())\n",
    "    y2 = int(ys.max())\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "\n",
    "def bbox_to_mask(bbox: List[int], shape: Tuple[int, int]) -> np.ndarray:\n",
    "    mask = np.zeros(shape, dtype=bool)\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    mask[y1 : y2 + 1, x1 : x2 + 1] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "# ===== 9. RLE Encoding =====\n",
    "def rle_encode(mask):\n",
    "    \"\"\"ë§ˆìŠ¤í¬ë¥¼ RLE í˜•ì‹ìœ¼ë¡œ ì¸ì½”ë”©\"\"\"\n",
    "    if torch.is_tensor(mask):\n",
    "        mask = mask.detach().cpu().numpy()\n",
    "\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[False], pixels, [False]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "# ===== 10. ë©”ì¸ íŒŒì´í”„ë¼ì¸ (Visual Focus) =====\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“ BBox ì˜ˆì¸¡ ì‹œì‘ (Visual Focus Refinement)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "bbox_dict = {}\n",
    "fallback_count = 0\n",
    "plural_debug_rows = []\n",
    "\n",
    "for img_id in tqdm(sorted(test_images.keys()), desc=\"BBox ì˜ˆì¸¡\"):\n",
    "    image = test_images[img_id]\n",
    "    query = queries.get(img_id, \"\")\n",
    "\n",
    "    if not query:\n",
    "        continue\n",
    "\n",
    "    bboxes, used_fallback = predict_bbox_refine(mllm_model, mllm_processor, image, query)\n",
    "    bbox_dict[img_id] = bboxes\n",
    "    if used_fallback:\n",
    "        fallback_count += 1\n",
    "    if is_plural_query(query):\n",
    "        plural_debug_rows.append({\n",
    "            \"id\": img_id,\n",
    "            \"query\": str(query),\n",
    "            \"num_boxes\": len(bboxes),\n",
    "            \"boxes\": json.dumps(bboxes),\n",
    "            \"used_fallback\": bool(used_fallback),\n",
    "        })\n",
    "\n",
    "print()\n",
    "print(f\"âœ… BBox ì˜ˆì¸¡ ì™„ë£Œ: {len(bbox_dict)}/{len(test_images)}\")\n",
    "print(f\"   Fallback ì‚¬ìš©: {fallback_count}ê°œ\")\n",
    "debug_df = pd.DataFrame(plural_debug_rows)\n",
    "debug_df.to_csv(PLURAL_DEBUG_CSV, index=False)\n",
    "print(f\"   Plural debug saved: {PLURAL_DEBUG_CSV} (rows: {len(debug_df)})\")\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ­ Mask ìƒì„± ì‹œì‘ (multimask)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "mask_dict = {}\n",
    "\n",
    "for img_id in tqdm(sorted(bbox_dict.keys()), desc=\"Mask ìƒì„±\"):\n",
    "    image = test_images[img_id]\n",
    "    query = queries.get(img_id, \"\")\n",
    "    bboxes = bbox_dict[img_id]\n",
    "    plural = is_plural_query(query)\n",
    "\n",
    "    try:\n",
    "        union_mask = np.zeros((image.size[1], image.size[0]), dtype=bool)\n",
    "        img_area = float(image.size[0] * image.size[1])\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            mask = generate_mask_with_sam(sam_predictor, image, bbox, use_points=plural)\n",
    "            box_area = max(1, (bbox[2] - bbox[0]) * (bbox[3] - bbox[1]))\n",
    "            mask_area = int(mask.sum())\n",
    "            ratio_box = mask_area / float(box_area)\n",
    "            ratio_img = mask_area / float(img_area) if img_area > 0 else 0.0\n",
    "\n",
    "            if ratio_box > NEGATIVE_OVERSEG_BOX_RATIO or ratio_img > NEGATIVE_OVERSEG_IMG_RATIO:\n",
    "                mask = bbox_to_mask(bbox, union_mask.shape)\n",
    "\n",
    "            union_mask |= mask\n",
    "\n",
    "        mask_dict[img_id] = union_mask\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Mask ì‹¤íŒ¨ ({img_id}): {e}\")\n",
    "\n",
    "print()\n",
    "print(f\"âœ… Mask ìƒì„± ì™„ë£Œ: {len(mask_dict)}/{len(bbox_dict)}\")\n",
    "\n",
    "\n",
    "# ===== 11. ì œì¶œ íŒŒì¼ ìƒì„± =====\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“„ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "submission_data = []\n",
    "\n",
    "for img_id in sorted(test_images.keys()):\n",
    "    if img_id in mask_dict:\n",
    "        mask = mask_dict[img_id]\n",
    "        rle_string = rle_encode(mask)\n",
    "    else:\n",
    "        rle_string = \"\"\n",
    "\n",
    "    submission_data.append({\n",
    "        \"ID\": img_id,\n",
    "        \"Label\": rle_string\n",
    "    })\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_path = OUTPUT_DIR / \"submission_challenge1_visual_focus.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥: {submission_path}\")\n",
    "print(f\"   ì´ {len(submission_df)}ê°œ ì˜ˆì¸¡\")\n",
    "print()\n",
    "print(\"íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "\n",
    "# ===== 12. ê²°ê³¼ ì‹œê°í™” =====\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š ê²°ê³¼ ì‹œê°í™” ì¤‘...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import random\n",
    "sample_ids = random.sample(list(mask_dict.keys()), min(4, len(mask_dict)))\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for idx, img_id in enumerate(sample_ids):\n",
    "    # ì›ë³¸\n",
    "    axes[0, idx].imshow(test_images[img_id])\n",
    "    axes[0, idx].set_title(f\"{img_id}\\n{queries[img_id][:30]}...\", fontsize=8)\n",
    "    axes[0, idx].axis('off')\n",
    "\n",
    "    # ë§ˆìŠ¤í¬\n",
    "    axes[1, idx].imshow(mask_dict[img_id], cmap='gray')\n",
    "    axes[1, idx].set_title(\"Mask\", fontsize=8)\n",
    "    axes[1, idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "viz_path = OUTPUT_DIR / \"visualization_final.png\"\n",
    "plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… ì‹œê°í™” ì €ì¥: {viz_path}\")\n",
    "\n",
    "\n",
    "# ===== 13. ìµœì¢… ìš”ì•½ =====\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ğŸ“ ì¶œë ¥: {OUTPUT_DIR}\")\n",
    "print(f\"ğŸ“„ ì œì¶œ íŒŒì¼: {submission_path}\")\n",
    "print(f\"ğŸ“Š ì‹œê°í™”: {viz_path}\")\n",
    "print()\n",
    "print(\"âš ï¸  ì œì¶œ ì‹œ ê¸°ë¡:\")\n",
    "print(f\"   - Seed: {SEED}\")\n",
    "print(\"   - do_sample: False âœ…\")\n",
    "print()\n",
    "print(\"ğŸ“Š ê²°ê³¼:\")\n",
    "print(f\"   - ì „ì²´: {len(test_images)}ì¥\")\n",
    "print(f\"   - BBox ì„±ê³µ: {len(bbox_dict)}ì¥\")\n",
    "print(f\"   - Fallback: {fallback_count}ì¥\")\n",
    "print(f\"   - Mask ìƒì„±: {len(mask_dict)}ì¥\")\n",
    "print()\n",
    "print(\"ğŸ‰ submission_challenge1_visual_focus.csvë¥¼ ì œì¶œí•˜ì„¸ìš”!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5283582-9e93-4d22-9ed7-cc3146e839c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Visualization (All Results)...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Visualize all masks from a submission CSV (RLE) in batches.\n",
    "\n",
    "Example:\n",
    "  python visualize_submission_masks.py \\\n",
    "    --submission /home/<user>/challenge1_outputs/submission_challenge1_final.csv \\\n",
    "    --dataset /home/jjs2403/2026_bootcamp_02/dataset/challenge_datasets/challenge1/test \\\n",
    "    --batch-size 4\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_test_images(test_dir: Path) -> dict:\n",
    "    patterns = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"]\n",
    "    image_files = []\n",
    "    for pat in patterns:\n",
    "        image_files.extend(sorted(test_dir.glob(pat)))\n",
    "    images = {}\n",
    "    for img_path in image_files:\n",
    "        img_id = img_path.stem\n",
    "        images[img_id] = Image.open(img_path).convert(\"RGB\")\n",
    "    return images\n",
    "\n",
    "\n",
    "def rle_decode(rle: str, shape: tuple) -> np.ndarray:\n",
    "    if not rle:\n",
    "        return np.zeros(shape, dtype=bool)\n",
    "    s = list(map(int, rle.split()))\n",
    "    starts = s[0::2]\n",
    "    lengths = s[1::2]\n",
    "    starts = [x - 1 for x in starts]\n",
    "    flat = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for st, ln in zip(starts, lengths):\n",
    "        flat[st : st + ln] = 1\n",
    "    return flat.reshape(shape).astype(bool)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    default_submission = Path.home() / \"challenge1_outputs\" / \"submission_challenge1_visual_focus.csv\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--submission\",\n",
    "        default=str(default_submission),\n",
    "        help=\"Path to submission CSV with columns: ID, Label\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        default=\"/home/jjs2403/2026_bootcamp_02/dataset/challenge_datasets/challenge1/test\",\n",
    "        help=\"Path to test images folder\",\n",
    "    )\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=4)\n",
    "    # Jupyter passes extra args (e.g., -f <kernel.json>), so ignore unknowns.\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    submission_path = Path(args.submission)\n",
    "    test_dir = Path(args.dataset)\n",
    "\n",
    "    if not submission_path.exists():\n",
    "        raise FileNotFoundError(f\"Submission not found: {submission_path}\")\n",
    "    if not test_dir.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found: {test_dir}\")\n",
    "\n",
    "    images = load_test_images(test_dir)\n",
    "    df = pd.read_csv(submission_path)\n",
    "\n",
    "    id_col = next((c for c in df.columns if c.lower() in (\"id\", \"image_id\", \"filename\")), df.columns[0])\n",
    "    label_col = next((c for c in df.columns if c.lower() in (\"label\", \"rle\")), df.columns[1])\n",
    "\n",
    "    mask_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        img_id = str(row[id_col]).replace(\".jpg\", \"\").replace(\".png\", \"\")\n",
    "        rle = \"\" if pd.isna(row[label_col]) else str(row[label_col])\n",
    "        if img_id not in images:\n",
    "            continue\n",
    "        w, h = images[img_id].size\n",
    "        mask_dict[img_id] = rle_decode(rle, (h, w))\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Visualization (All Results)...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    all_ids = sorted(list(mask_dict.keys()))\n",
    "    batch_size = max(1, args.batch_size)\n",
    "\n",
    "    for i in range(0, len(all_ids), batch_size):\n",
    "        batch_ids = all_ids[i : i + batch_size]\n",
    "        curr_batch_len = len(batch_ids)\n",
    "\n",
    "        fig, axes = plt.subplots(2, curr_batch_len, figsize=(4 * curr_batch_len, 6))\n",
    "        if curr_batch_len == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "\n",
    "        for idx, img_id in enumerate(batch_ids):\n",
    "            axes[0, idx].imshow(images[img_id])\n",
    "            axes[0, idx].set_title(f\"{img_id}\", fontsize=10)\n",
    "            axes[0, idx].axis(\"off\")\n",
    "\n",
    "            axes[1, idx].imshow(mask_dict[img_id], cmap=\"gray\")\n",
    "            axes[1, idx].set_title(\"Mask\", fontsize=10)\n",
    "            axes[1, idx].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221b76f-75d1-40b1-8240-075431330b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BBox Visualization (Inline Grid) =====\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def _normalize_bboxes(bbox_val):\n",
    "    if bbox_val is None:\n",
    "        return []\n",
    "    # dict style\n",
    "    if isinstance(bbox_val, dict) and 'bbox_2d' in bbox_val:\n",
    "        bbox_val = bbox_val['bbox_2d']\n",
    "    # list of boxes or single box\n",
    "    boxes = []\n",
    "    if isinstance(bbox_val, (list, tuple)):\n",
    "        # nested list: [[x1,y1,x2,y2], ...]\n",
    "        if len(bbox_val) > 0 and isinstance(bbox_val[0], (list, tuple)):\n",
    "            for b in bbox_val:\n",
    "                if isinstance(b, (list, tuple)) and len(b) >= 4:\n",
    "                    x1, y1, x2, y2 = b[:4]\n",
    "                    boxes.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "        # flat list: [x1,y1,x2,y2]\n",
    "        elif len(bbox_val) >= 4:\n",
    "            x1, y1, x2, y2 = bbox_val[:4]\n",
    "            boxes.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "    return boxes\n",
    "\n",
    "cols = 4\n",
    "max_viz = 129  # change if needed\n",
    "viz_ids = list(bbox_dict.keys())[:max_viz]\n",
    "\n",
    "# filter valid\n",
    "valid = []\n",
    "for img_id in viz_ids:\n",
    "    bbs = _normalize_bboxes(bbox_dict.get(img_id))\n",
    "    if not bbs:\n",
    "        continue\n",
    "    valid.append((img_id, bbs))\n",
    "\n",
    "if not valid:\n",
    "    print('No valid bboxes to display.')\n",
    "else:\n",
    "    rows = (len(valid) + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    if rows == 1:\n",
    "        axes = [axes]\n",
    "    axes = [ax for row in axes for ax in (row if isinstance(row, (list, tuple, np.ndarray)) else [row])]\n",
    "\n",
    "    for idx, (img_id, bboxes) in enumerate(valid):\n",
    "        ax = axes[idx]\n",
    "        image = test_images[img_id]\n",
    "        ax.imshow(image)\n",
    "        for b in bboxes:\n",
    "            x1, y1, x2, y2 = b\n",
    "            rect = patches.Rectangle((x1, y1), (x2 - x1 + 1), (y2 - y1 + 1),\n",
    "                                     linewidth=2.0, edgecolor='lime', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "        ax.set_title(f'{img_id} (boxes: {len(bboxes)})', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # turn off unused axes\n",
    "    for j in range(len(valid), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f'âœ… Displayed {len(valid)} bbox visualizations inline in a grid.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e9bfd-8e1c-4e73-bc9f-da60e08bb8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfa936-ea38-4a6b-92d5-bc58e09cb654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bootcamp Environment 1",
   "language": "python",
   "name": "bootcamp_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
